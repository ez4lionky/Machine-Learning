1. The selected 18 features can be divided into three categories:
The frequency of  words 'he she the of how a had could' and 9 words 'miss one mr. like said upon mrs. old holmes' selected according to the max value of tf-idf (each author three words), and the average length of word for each doc to be predicted.
The frequency of function words and the average length of word for each doc can reflect the author's writing style. Words with the highest tf-idf value can be considered as keywords for each author's book.

However, in the decision tree (dt) model, if there are too many features , the dt will be overfitting and use too many irrelevant input feature. In the following experiment results, will compare the relationship between feature number and model performance.

2. In the training process of decision tree, we tune the information gain threshold and the depth under the same data set, but with the different features.

a. Original 18 features.
The results are reported in following table:
depth	threshold	accuracy
4		    0.005		 0.7183
6		    0.005		 0.6750
8		    0.005		 0.7217
10		    0.005		 0.6800
12		    0.005		 0.6767

4		     0.01		 0.7183
6		     0.01		 0.6716
8		     0.01		 0.7217
10		     0.01		 0.6750
12		     0.01		 0.6783

First, fix the threshold at 0.005, then tune the depth. When the number of depth increases until 8, the accuracy is increasing, after then, the performance drops.
Then, we tune the threshold to 0.01, and redo the experiment.

b. Delete some functional words - 'the of how a had could', that is only left 12 features.
depth	threshold	accuracy
4		    0.005		 0.7183
6		    0.005		 0.6750
8		    0.005		 0.7350
10		    0.005		 0.7367
12		    0.005		 0.7300

4		     0.01		 0.7183
6		     0.01		 0.7250
8		     0.01		 0.7333
10		     0.01		 0.7400
12		     0.01		 0.7250

Follow the previous configuration, we redo the experiment and reported accuracy results in the table.
Comparing the results of the two experiments, we can see that the performance of dt is not good when the number of features is too large. Because the dt will be overfitting and use too many irrelevant input feature.

In the final, we choose the best model with the depth 10, threshold 0.01.

3. In the training process of perceptron, we tune the learning rate and epochs.
epoch learning rate	accuracy
1000    0.01 			0.7217
3000    0.01			0.7483333333333333
5000    0.01			0.7600
10000  0.01			0.7617

1000    0.001 		0.7217
3000    0.001			0.7483
5000    0.001			0.7600
10000  0.001		0.7667

Since the accuracy of the model has been increasing when the model epoch number is 1000, 3000, 5000. Therefore, the epoch number is gradually increased until the model performance is converged, i. e. epoch 10000. And from the accuracy-epochs curve figure, we can see the perceptron model gradually converge.

Figure1.

